{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Clone repos from github","metadata":{}},{"cell_type":"code","source":"%%capture\n!git clone https://github.com/hukenovs/hagrid.git\n# or mirror link:\n%cd hagrid\n# Install requirements\n%pip install -r requirements.txt --user\n%cd ..\n# !wget https://raw.githubusercontent.com/shitkov/signature_detector/main/dataset.zip\n!git clone https://github.com/ultralytics/yolov5\n%cd yolov5\n%pip install -qr requirements.txt\n# !unzip /content/dataset.zip -d /content/yolov5/\n%cd ..","metadata":{"id":"mzl9mnLrvLfp","execution":{"iopub.status.busy":"2022-10-09T16:40:36.829310Z","iopub.execute_input":"2022-10-09T16:40:36.830308Z","iopub.status.idle":"2022-10-09T16:40:59.126780Z","shell.execute_reply.started":"2022-10-09T16:40:36.830203Z","shell.execute_reply":"2022-10-09T16:40:59.125308Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## YOLOv5 habr","metadata":{"id":"ykt4DSstyIJA"}},{"cell_type":"code","source":"%%capture\n# !wget https://raw.githubusercontent.com/shitkov/signature_detector/main/dataset.zip\n!git clone https://github.com/ultralytics/yolov5\n%cd yolov5\n%pip install -qr requirements.txt\n# !unzip /content/dataset.zip -d /content/yolov5/\n%cd ..","metadata":{"id":"VE-8E7yTyHMb","execution":{"iopub.status.busy":"2022-10-09T16:40:59.133150Z","iopub.execute_input":"2022-10-09T16:40:59.135555Z","iopub.status.idle":"2022-10-09T16:41:09.911812Z","shell.execute_reply.started":"2022-10-09T16:40:59.135510Z","shell.execute_reply":"2022-10-09T16:41:09.910527Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing annotation files to YOLOv5 format","metadata":{}},{"cell_type":"code","source":"import json\nimport os\n\nYOLO_ANN_TRAIN_JSON_PATH = '/kaggle/input/hagrid-call-test/ann_train_val/ann_train_val/call.json'\nYOLO_ANN_TEST_JSON_PATH  = '/kaggle/input/hagrid-call-test/ann_test/ann_test/call.json'\nYOLO_ANN_TRAIN_TXTS_PATH = '/kaggle/working/labels_train'\nYOLO_ANN_TEST_TXTS_PATH  = '/kaggle/working/labels_test'\nHAGRID_PHOTO_TRAIN_PATH  = '/kaggle/input/hagrid-call-test/train_val_call'\nHAGRID_PHOTO_TEST_PATH   = '/kaggle/input/hagrid-call-test/test/call'\n\nYOLO_ANN_FAKES_JSON_PATH = '/kaggle/input/hagrid-call-test/ann_test/ann_test'\nYOLO_ANN_FAKES_TXTS_PATH = '/kaggle/working/labels_fakes'\nHAGRID_PHOTO_FAKES_PATH  = '/kaggle/input/hagrid-call-test/test'\n\n\ndef preprocess_json2txt(json_path:str, txt_path:str, photo_path:str) -> None:\n    dct={}\n    with open(json_path) as f:\n        d = json.load(f)\n        for key in d:\n            dct[key] = d[key]\n\n    if not(os.path.isdir(txt_path)):\n        os.mkdir(txt_path)\n\n    filenames = []\n    for (dirpath, dirnames, f) in os.walk(photo_path):\n        filenames.extend(f)\n        break\n\n    for filename in filenames:\n        name = filename.split('.')[0]\n        with open(os.path.join(txt_path, name+'.txt'), 'w') as f:\n            lines = []\n            for i in range(len(dct[name]['bboxes'])):\n                label = (1 if dct[name]['labels'][i]=='call' else 0)\n                bbox = [dct[name]['bboxes'][i][0]+0.5*dct[name]['bboxes'][i][2],\n                        dct[name]['bboxes'][i][1]+0.5*dct[name]['bboxes'][i][3],\n                        dct[name]['bboxes'][i][2],\n                        dct[name]['bboxes'][i][3]]\n                lines.append(str(label) + ' ' + ' '.join(str(x) for x in bbox) + '\\n')\n            f.writelines(lines)\n            \npreprocess_json2txt(YOLO_ANN_TRAIN_JSON_PATH, YOLO_ANN_TRAIN_TXTS_PATH, HAGRID_PHOTO_TRAIN_PATH)\npreprocess_json2txt(YOLO_ANN_TEST_JSON_PATH,  YOLO_ANN_TEST_TXTS_PATH,  HAGRID_PHOTO_TEST_PATH )\n\nfakes_names=['dislike','fist','four','like','mute','ok','one','palm','peace','peace_inverted','rock',\\\n             'stop','stop_inverted','three','three2','two_up','two_up_inverted']\n\nfor name in fakes_names:\n    preprocess_json2txt(os.path.join(YOLO_ANN_FAKES_JSON_PATH, name+'.json'),\\\n                        YOLO_ANN_FAKES_TXTS_PATH,\\\n                        os.path.join(HAGRID_PHOTO_FAKES_PATH, name))","metadata":{"id":"BKOi6CJT_5wu","execution":{"iopub.status.busy":"2022-10-09T16:41:09.914033Z","iopub.execute_input":"2022-10-09T16:41:09.914452Z","iopub.status.idle":"2022-10-09T16:42:18.673852Z","shell.execute_reply.started":"2022-10-09T16:41:09.914411Z","shell.execute_reply":"2022-10-09T16:42:18.672858Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"### OBSOLETE\ndef create_dataset(dataset_path:str, img_train_path:str, label_train_path:str, valid_size:float,\n                  random_state:int):\n    np.random.seed(random_state)\n    dirpath=''\n    images = []\n    for (dirpath_, dirpaths, filenames) in os.walk(img_train_path):\n        images.extend(filenames)\n        dirpath += dirpath_\n        break\n        \n    images.sort()\n    n = len(images)\n    train_indices = np.arange(n)\n    np.random.shuffle(train_indices)\n    train_indices = train_indices[:n]\n    mask = np.zeros(n, dtype=np.bool)\n    for i in train_indices:\n        mask[i]=1\n    \n    for i,image in enumerate(images):\n        name = image.split('.')[0]\n        dir_type = ('train' if i in mask[i]==1 else 'valid')\n        exec(f'!cp -s {os.path.join(img_train_path, image)}' + \\\n             f'{os.path.join(dataset_path, \"images\", dir_type, image)}')\n        txt = name+'.txt'\n        exec(f'!cp -s {os.path.join(label_train_path, txt)}' + \\\n             f'{os.path.join(dataset_path, \"labels\", dir_type, txt)}')\n        \n# create_dataset('/kaggle/temp/dataset', HAGRID_PHOTO_TRAIN_PATH, YOLO_ANN_TRAIN_TXTS_PATH, valid_size=0.05, random_state=1)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-10-09T16:42:18.676290Z","iopub.execute_input":"2022-10-09T16:42:18.676571Z","iopub.status.idle":"2022-10-09T16:42:18.685285Z","shell.execute_reply.started":"2022-10-09T16:42:18.676546Z","shell.execute_reply":"2022-10-09T16:42:18.684373Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /kaggle/temp/dataset/images\n!mkdir -p /kaggle/temp/dataset/labels\n%cd ../temp\n!cp -r -s '/kaggle/input/hagrid-call-test/train_val_call' './dataset/images'\n!mv './dataset/images/train_val_call' './dataset/images/train'\n!cp -r -s '/kaggle/input/hagrid-call-test/test/call'      './dataset/images'\n!mv './dataset/images/call' './dataset/images/valid'\n!cp -r -s '/kaggle/working/labels_train' './dataset/labels'\n!mv './dataset/labels/labels_train' './dataset/labels/train'\n!cp -r -s '/kaggle/working/labels_test' './dataset/labels'\n!mv './dataset/labels/labels_test' './dataset/labels/valid'\n%cd ../working","metadata":{"execution":{"iopub.status.busy":"2022-10-09T16:42:18.686969Z","iopub.execute_input":"2022-10-09T16:42:18.687680Z","iopub.status.idle":"2022-10-09T16:42:44.544650Z","shell.execute_reply.started":"2022-10-09T16:42:18.687644Z","shell.execute_reply":"2022-10-09T16:42:44.543284Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/temp\n/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"from multiprocessing import Pool\n\ndef _scan_folder(dataset_path, dirs):\n    lists = {}\n    s1='images'\n    for s2 in dirs:\n        lists[s2] = []\n        cur_path = os.path.join(dataset_path, s1, s2)\n        for (dirpath, dirpaths, filenames) in os.walk(cur_path):\n                for i in range(len(filenames)):\n                    filenames[i] = filenames[i].split('.')[0]\n                lists[s2].extend(sorted(filenames))\n    return lists\n\ndef _trim_dataset_helper(dataset_path:str, s2:str, name:str):\n    os.remove(os.path.join(dataset_path, 'images', s2, name + '.jpg'))\n    os.remove(os.path.join(dataset_path, 'labels', s2, name + '.txt'))\n    \ndef trim_dataset(dataset_path:str, n_train:int, n_valid:int, batch_index=0):\n    dirs = [['images','labels'], ['train','valid']]\n    lists = _scan_folder(dataset_path, dirs[1]) \n    \n    for s2 in dirs[1]:\n        n = (n_train if s2=='train' else n_valid)\n        filenames = lists[s2].copy()\n        _start = (batch_index * n) % (len(filenames))\n        _end   = _start + n\n        filenames = ((filenames[:_start] + filenames[_end:]) if _end <= len(filenames) else \\\n                      filenames[_end%len(filenames):_start])\n            \n        with Pool(4) as p:\n            p.starmap(_trim_dataset_helper, [(dataset_path, s2, name) for name in filenames])\n                \n    lists = _scan_folder(dataset_path, dirs[1])\n    return lists","metadata":{"execution":{"iopub.status.busy":"2022-10-09T16:42:44.550326Z","iopub.execute_input":"2022-10-09T16:42:44.551237Z","iopub.status.idle":"2022-10-09T16:42:44.591347Z","shell.execute_reply.started":"2022-10-09T16:42:44.551184Z","shell.execute_reply":"2022-10-09T16:42:44.590178Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def _copy_fakes_helper(dataset_path:str, image_path:str, label_path:str, filename:str, dir_type:str):\n    filename = filename.split('.')[0]\n    os.popen(f\"cp -s {os.path.join(image_path, filename+'.jpg')} \" + \\\n             f\"{os.path.join(dataset_path, 'images', dir_type)}\")\n    os.popen(f\"cp -s {os.path.join(label_path, filename+'.txt')} \" + \\\n             f\"{os.path.join(dataset_path, 'labels', dir_type)}\") \n    \ndef copy_fakes(dataset_path:str, image_path:str, label_path:str, n_train:int,\n               n_valid:int, fakes_names:list, batch_index=0):\n    \n    n_train = n_train//len(fakes_names)\n    n_valid = n_valid//len(fakes_names)\n    \n    for name in fakes_names:\n        dir_path = os.path.join(image_path,name)\n        filenames = sorted(os.listdir(dir_path))\n        _start = (batch_index * (n_train + n_valid)) % (len(filenames))\n        _end   = _start + (n_train + n_valid)\n        filenames = (filenames[_start:_end] if _end <= len(filenames) else \\\n                     filenames[_start:] + filenames[:_end%len(filenames)])\n        \n        with Pool(4) as p:\n            p.starmap(_copy_fakes_helper, [(dataset_path, dir_path, label_path, filename, 'train')\\\n                                           for filename in filenames[:n_train]])\n        with Pool(4) as p:\n            p.starmap(_copy_fakes_helper, [(dataset_path, dir_path, label_path, filename, 'valid')\\\n                                           for filename in filenames[n_train:n_train+n_valid]])\n        \n    lists = _scan_folder(dataset_path, ['train','valid'])\n    return lists","metadata":{"execution":{"iopub.status.busy":"2022-10-09T16:42:44.604477Z","iopub.execute_input":"2022-10-09T16:42:44.607536Z","iopub.status.idle":"2022-10-09T16:42:45.848168Z","shell.execute_reply.started":"2022-10-09T16:42:44.607491Z","shell.execute_reply":"2022-10-09T16:42:45.847093Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# _scan_folder('/kaggle/temp/dataset', [['images','labels'], ['train','valid']])","metadata":{"execution":{"iopub.status.busy":"2022-10-09T16:42:45.849683Z","iopub.execute_input":"2022-10-09T16:42:45.850210Z","iopub.status.idle":"2022-10-09T16:42:45.862506Z","shell.execute_reply.started":"2022-10-09T16:42:45.850146Z","shell.execute_reply":"2022-10-09T16:42:45.861427Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Trimming dataset","metadata":{}},{"cell_type":"code","source":"N_TRAIN = 5000\nN_VALID = 500\nBATCH_INDEX=1\n\ndataset_files = trim_dataset('/kaggle/temp/dataset', N_TRAIN//2, N_VALID//2, BATCH_INDEX)\ndataset_files = copy_fakes('/kaggle/temp/dataset', HAGRID_PHOTO_FAKES_PATH, YOLO_ANN_FAKES_TXTS_PATH,\n                          N_TRAIN//2, N_VALID//2, fakes_names, BATCH_INDEX)\nprint(f\"Batch index: {BATCH_INDEX}\" + \\\n      f\"Train dataset length: {len(dataset_files['train'])}\\n\" + \\\n      f\"Valid dataset length: {len(dataset_files['valid'])}\")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T16:42:45.864278Z","iopub.execute_input":"2022-10-09T16:42:45.865016Z","iopub.status.idle":"2022-10-09T16:43:44.497987Z","shell.execute_reply.started":"2022-10-09T16:42:45.864981Z","shell.execute_reply":"2022-10-09T16:43:44.496807Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Batch index: 1Train dataset length: 4999\nValid dataset length: 488\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('./sig.yaml', 'w') as f:\n    f.write('train: /kaggle/temp/dataset/images/train/\\n' + \\\n            'val: /kaggle/temp/dataset/images/valid/\\n' + \\\n            '\\n' + \\\n            'nc: 2\\n' + \\\n            \"names: ['no_gesture','call']\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-10-09T16:43:44.501977Z","iopub.execute_input":"2022-10-09T16:43:44.502295Z","iopub.status.idle":"2022-10-09T16:43:44.513203Z","shell.execute_reply.started":"2022-10-09T16:43:44.502263Z","shell.execute_reply":"2022-10-09T16:43:44.510678Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"!cat ./sig.yaml","metadata":{"execution":{"iopub.status.busy":"2022-10-09T16:43:44.515614Z","iopub.execute_input":"2022-10-09T16:43:44.515999Z","iopub.status.idle":"2022-10-09T16:43:45.463120Z","shell.execute_reply.started":"2022-10-09T16:43:44.515963Z","shell.execute_reply":"2022-10-09T16:43:45.461949Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"train: /kaggle/temp/dataset/images/train/\nval: /kaggle/temp/dataset/images/valid/\n\nnc: 2\nnames: ['no_gesture','call']\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom IPython.display import Image, clear_output\nfrom gc import collect\ncollect()","metadata":{"id":"KFeBQo73VwWC","execution":{"iopub.status.busy":"2022-10-09T16:43:45.464911Z","iopub.execute_input":"2022-10-09T16:43:45.465549Z","iopub.status.idle":"2022-10-09T16:43:47.149566Z","shell.execute_reply.started":"2022-10-09T16:43:45.465505Z","shell.execute_reply":"2022-10-09T16:43:47.148649Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"markdown","source":"## Download weights","metadata":{}},{"cell_type":"code","source":"!wget https://github.com/SpryGorgon/YOLOv5/raw/main/yolov5s_1280_20.pt","metadata":{"execution":{"iopub.status.busy":"2022-10-09T16:43:47.150880Z","iopub.execute_input":"2022-10-09T16:43:47.151972Z","iopub.status.idle":"2022-10-09T16:43:49.511266Z","shell.execute_reply.started":"2022-10-09T16:43:47.151934Z","shell.execute_reply":"2022-10-09T16:43:49.509882Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"--2022-10-09 16:43:47--  https://github.com/SpryGorgon/YOLOv5/raw/main/yolov5s_1280_20.pt\nResolving github.com (github.com)... 140.82.114.4\nConnecting to github.com (github.com)|140.82.114.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/SpryGorgon/YOLOv5/main/yolov5s_1280_20.pt [following]\n--2022-10-09 16:43:48--  https://raw.githubusercontent.com/SpryGorgon/YOLOv5/main/yolov5s_1280_20.pt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 14624117 (14M) [application/octet-stream]\nSaving to: ‘yolov5s_1280_20.pt’\n\nyolov5s_1280_20.pt  100%[===================>]  13.95M  --.-KB/s    in 0.09s   \n\n2022-10-09 16:43:49 (156 MB/s) - ‘yolov5s_1280_20.pt’ saved [14624117/14624117]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training YOLOv5","metadata":{}},{"cell_type":"code","source":"%cd yolov5\n!python train.py --img 1280 --batch -1 --epochs 10 --data '../sig.yaml' --cfg yolov5s.yaml --weights ../yolov5s_1280_20.pt --cache disk --device 0\n%cd ..","metadata":{"id":"MT6Fw41HNamX","outputId":"b588a0be-2ded-4fe1-954c-9801e86d27ca","execution":{"iopub.status.busy":"2022-10-09T09:24:11.374573Z","iopub.execute_input":"2022-10-09T09:24:11.374907Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"/kaggle/working/yolov5\n\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n\u001b[34m\u001b[1mtrain: \u001b[0mweights=../yolov5s_1280_10.pt, cfg=yolov5s.yaml, data=../sig.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=-1, imgsz=1280, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=disk, image_weights=False, device=0, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\nYOLOv5 🚀 v6.2-187-g5ef69ef Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\n\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs in Weights & Biases\n\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\nDownloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n100%|████████████████████████████████████████| 755k/755k [00:00<00:00, 16.9MB/s]\nOverriding model.yaml nc=80 with nc=2\n\n                 from  n    params  module                                  arguments                     \n  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\nYOLOv5s summary: 214 layers, 7025023 parameters, 7025023 gradients, 16.0 GFLOPs\n\nTransferred 348/349 items from ../yolov5s_1280_10.pt\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for --imgsz 1280\n\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (Tesla P100-PCIE-16GB) 15.90G total, 0.10G reserved, 0.05G allocated, 15.75G free\n      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n     7025023       63.82         0.816         75.99         58.86      (1, 3, 1280, 1280)                    list\n     7025023       127.6         1.504         44.68          83.5      (2, 3, 1280, 1280)                    list\n     7025023       255.3         2.915         68.64         150.3      (4, 3, 1280, 1280)                    list\n     7025023       510.5         5.996         118.5         190.9      (8, 3, 1280, 1280)                    list\n     7025023        1021        12.554         238.4         329.3     (16, 3, 1280, 1280)                    list\n\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 16 for CUDA:0 12.60G/15.90G (79%) ✅\n\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/kaggle/temp/dataset/labels/train' images and labels...4999 fou\u001b[0m\n\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/temp/dataset/labels/train.cache\n\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (37.7GB disk): 100%|██████████| 4999/4999 [07:03<00:00, 11\u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mScanning '/kaggle/temp/dataset/labels/valid' images and labels...488 found,\u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/temp/dataset/labels/valid.cache\n\u001b[34m\u001b[1mval: \u001b[0mCaching images (3.7GB disk): 100%|██████████| 488/488 [00:33<00:00, 14.57it\u001b[0m\n\n\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.83 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\nPlotting labels to runs/train/exp/labels.jpg... \nImage sizes 1280 train, 1280 val\nUsing 2 dataloader workers\nLogging results to \u001b[1mruns/train/exp\u001b[0m\nStarting training for 10 epochs...\n\n      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n        0/9      12.4G     0.0189   0.008934    0.00121         34       1280:  ","output_type":"stream"}]},{"cell_type":"code","source":"!cp './yolov5/runs/train/exp/weights/best.pt' -r '/kaggle/working/yolov5s_1280_30.pt'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Validating ","metadata":{}},{"cell_type":"code","source":"%cd yolov5\n!python val.py --weights './runs/train/exp/weights/best.pt' --data '../sig.yaml' --img 1280 --device 0\n%cd ..","metadata":{"id":"NrWsa07POIVj","outputId":"26c7b0a6-1cfe-4283-edd8-d8d770153e28","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\n# img = Image.open('yolov5/runs/val/exp/val_batch2_pred.jpg')\n# img.show()\n\n\n# model = torch.hub.load('yolov5', 'custom', path='yolov5n_640_30.pt', source='local')\n# imgs = ['/kaggle/temp/dataset/images/valid/' + x + '.jpg' for x in dataset_files['valid']]\n# results = model(['/kaggle/temp/dataset/images/valid/e9524db2-1590-4c77-a084-ac217907e56f.jpg'])\n# results.print()\n# results.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Detecting","metadata":{}},{"cell_type":"code","source":"%cd yolov5\n!python detect.py --weights './runs/train/exp/weights/best.pt' --imgsz 1280 --device 0 --source /kaggle/temp/dataset/images/valid\n%cd ..","metadata":{"execution":{"iopub.status.busy":"2022-10-09T16:46:17.204338Z","iopub.execute_input":"2022-10-09T16:46:17.204778Z","iopub.status.idle":"2022-10-09T16:46:24.695026Z","shell.execute_reply.started":"2022-10-09T16:46:17.204726Z","shell.execute_reply":"2022-10-09T16:46:24.693856Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"/kaggle/working/yolov5\n\u001b[34m\u001b[1mdetect: \u001b[0mweights=['../yolov5s_1280_20.pt'], source=/kaggle/input/hagrid-call-test/test, data=data/coco128.yaml, imgsz=[1280, 1280], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=0, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\nYOLOv5 🚀 v6.2-187-g5ef69ef Python-3.7.12 torch-1.11.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\nFusing layers... \nYOLOv5s summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\nTraceback (most recent call last):\n  File \"detect.py\", line 258, in <module>\n    main(opt)\n  File \"detect.py\", line 253, in main\n    run(**vars(opt))\n  File \"/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n    return func(*args, **kwargs)\n  File \"detect.py\", line 108, in run\n    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n  File \"/kaggle/working/yolov5/utils/dataloaders.py\", line 270, in __init__\n    assert self.nf > 0, f'No images or videos found in {p}. ' \\\nAssertionError: No images or videos found in /kaggle/input/hagrid-call-test/test. Supported formats are:\nimages: ('bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp', 'pfm')\nvideos: ('asf', 'avi', 'gif', 'm4v', 'mkv', 'mov', 'mp4', 'mpeg', 'mpg', 'ts', 'wmv')\n/kaggle/working\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Saving runs output to .tar","metadata":{}},{"cell_type":"code","source":"!tar -czf runs_output.zip './yolov5/runs'","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}